---
title: "Information & Learning Lecture 1 Review"
categories:
  - lectures
tags:
  - lectures
toc: true
toc_sticky: true
toc_label: "Lecture Review"
date: 2024-07-30 17:30:00 +0900
author_profile: false
header:
  overlay_image: # assets/images/pages/2024-07-20.png
  overlay_filter: 0.5 
  teaser: # assets/images/pages/2024-07-20.png
use_math: true
# published: false
---
ë‹¨ê¸° íŠ¹ê°• ë¹ ë¥´ê²Œ ì •ë¦¬ !    
 
## 1 Information Theory ê¸°ì´ˆ
ìµœëŒ€í•œ ë§ì€ ë°ì´í„°ë¥¼ ë§¤ì²´ì— ì €ì¥í•˜ê±°ë‚˜ ì±„ë„ì„ í†µí•´ í†µì‹ í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì •ëŸ‰í™”í•˜ëŠ” ì‘ìš© ìˆ˜í•™ì˜ í•œ ë¶„ì•¼ë¡œ, ìˆ˜í•™ì  ëª¨ë¸ì€ í¬ê²Œ ì†ŒìŠ¤ ì½”ë”© ì´ë¡ , ì±„ë„ ì½”ë”© ì´ë¡ , ë¶€í˜¸ìœ¨-ë³€í˜• ì´ë¡  3ê°€ì§€ê°€ ìˆë‹¤.    

### Entropy ë€?   
random eventì— ëŒ€í•´ì„œ, information quantityë¥¼ Entropyë¼ ë¶€ë¥¸ë‹¤.   

**"How can we measure the uncertainty mathematically?"**   
information entropyê°€ ì»¤ì§ˆìˆ˜ë¡ uncertaintyê°€ ì»¤ì§„ë‹¤.   

- Hartley Measure    
: ê°€ëŠ¥í•œ ì‚¬ê±´ì˜ ìˆ˜ë§Œì„ ê³ ë ¤   
$$H = \log_b (\# of Possibilities)$$    
- Shannon's Measure   
: ì‚¬ê±´ì˜ í™•ë¥ ë¶„í¬ ê³ ë ¤
Xê°€ PMF $P_X$ì™€ í•¨ê»˜ RVë¼ í•œë‹¤.   
$$H(X)\equiv E[-\log_b P_x(X)] \\ = \sum_{a \in supp(P_X)} -P_X(a)\log_b P_X(X)$$   
(* UNIT : b = 2 -> bits, b = e -> nats)   
- RÃ©nyi Entropy   
$$\displaystyle H_\alpha(X) = \frac{1 - \alpha}{\alpha} \log \left( \sum_i P(x_i)^\alpha \right)$$   

- ì„±ì§ˆ : $0\le H(X) \le \log_2 L$   

*ì¦ëª… ìƒëµ   

### Joint Entropy
$$
H(X,Y) = E[- \log_2 P_XY(X,Y)] \\
= \displaystyle \sum_{[b.c] \in supp(P_XY)} -P_XY(b, c) \log_2 P_{XY}(b,c)
$$   

### Conditional Entropy
$$
\displaystyle H(X|Y=b) = \sum_{a \in supp(P_{X|Y}(\cdot|b))} -P_XY(a|b) \log_2 P_{X|Y}(a|b)
$$   
- ì„±ì§ˆ 
  - $ H(X|Y) = H(X, Y) - H(Y) $    
  - $ I(X; Y) = H(Y) - H(Y|X) $    
  - $ I(X; Y) = H(X) + H(Y) - H(X, Y) $   
  - $ I(X; Y) = I(Y; X) $   
  - $  I(X; X) = H(X) $   
  - $  0 \leq I(X; Y) \leq \min(H(X), H(Y)) $   

*ì¦ëª… ìƒëµ   

### Relative Entropy   
Informational Divergence or Kullback-Leibler Distance   
$$
D(P_X||P_Y) = \displaystyle \sum_{a \in supp(P_X)} P_X(a) \log_2 \frac{P_X(a)}{P_Y(a)}\\
= E[\log_2 \frac{P_X(X)}{P_Y(X)}]
$$   

*ì¦ëª… ìƒëµ   

### Mutual Information   
ë‘ í™•ë¥  ë³€ìˆ˜ ì‚¬ì´ì˜ ìƒí˜¸ ì˜ì¡´ì„±   
í•˜ë‚˜ì˜ RVê°€ ë‹¤ë¥¸ RVì— ëŒ€í•´ ê°–ê³  ìˆëŠ” ì •ë³´ì˜ ì–‘   
$$I(X;Y) = D(P_{XY}||P_XP_Y) $$   
- ì„±ì§ˆ : $I(X;Y) \ge 0$   

### Conditional Mutual Information
$$I(X; Y|Z) = H(X|Z) - H(X|YZ)$$   

### Data Processing Inequality
"system"ì„ source $P_X$ ---$X$---> channel $P_{Y|X}$ ---$Y$---> channel $P_{X|Y}$---> $Z$ ë¼ê³  í•˜ì.   
ë§Œì•½ X-Y-Zê°€ Markov chainì„ í˜•ì„±í•œë‹¤ë©´,   
$$I(X; Z) \leq I(X; Y)$$   
$$I(Y; Z) \leq I(X; Y)$$   

*ì¦ëª… ìƒëµ   

### Fano's Inequality   
"system"ì„ ---$X$---> system $P_{\hat{X}|X}$ ---$\hat{X}$---> ë¼ê³  í•˜ì.   
Error Prob : $P_e = Pr[X \ne \hat{X}]$   
$$H(X|\hat{X}) \le H_2(P_e)+P_e\log_2(|\chi|-1)$$   

ë§Œì•½ $\hat{X}$ê°€ X-Y-$\hat{X}$ ì˜ estimatorì´ê³  $P_e = Pr[X \ne \hat{X}]$ ë¼ë©´,   
$$H(X|Y) \le H(X|\hat{X}) \le H_2(P_e)+P_e\log_2(|\chi|-1)$$    
ì¦‰, Data-processing inequality.   

*ì¦ëª… ìƒëµ   

### Asymptotic Equipartition Property (AEP)
$$|- \frac{1}{n} \log_b P_X(X^n) - H(X)| < \epsilon$$     
1. $2^{n [H(X) - \epsilon]} < P_X(x^n) < 2^{n [H(X) + \epsilon]}$    
2. $\Pr(x^n \in \mathcal{A}_\epsilon^{(n)}) \geq 1 - \epsilon$    
3. $1 - \epsilon < \frac{|\mathcal{A}_\epsilon^{(n)}|}{2^n H(X)} < 1 + \epsilon $    

### Data Compression
í™•ë¥  ë³€ìˆ˜ Mì€ $X^n$ì— ì˜ì¡´í•œë‹¤. (block-to-variable length encoder)    
rate : $\frac{E[M]}{n}$   
$$\frac{E[M]}{n}=H(X)+\epsilon \text{  for any }\epsilon > 0$$   
- Source Coding Theorem : 

### Channel Capacity   
- Uniformly Dispersive Channels   
- Uniformly Focusing Channels   
- Symmetric Channels   
$$
C = \displaystyle \max_{P_{X}} I(X; Y) \leq \max_{P_{X}} H(X) \leq \log_2 J$$    
$$
\displaystyle C \leq \max_{P_{X}} H(Y) \leq \log_2 K
$$        
- Channel Coding Theorem : $ C= \displaystyle \max_{P_X(\cdot)} I(X;Y)$   

*ì¦ëª… ìƒëµ   

---

- Channel Capacity ì„±ì§ˆ
  $C \triangleq \max_{p_X} I(X; Y)$ ë¼ê³  í•  ë•Œ,   
  - $ C \geq 0  \text{ since } I(X; Y) \geq 0  $   
  - $ C \leq \log |\mathcal{X}| \text{ since } 
    C = \displaystyle \max_{p(x)} I(X; Y) \leq \max_{p(x)} H(X) = \log |\mathcal{X}|
    $     
  - $ ğ¼(ğ‘‹; ğ‘Œ)\text{ is a continuous function of }ğ‘(ğ‘¥). $    
  - $ ğ¼(ğ‘‹; ğ‘Œ)\text{ is a concave function of }ğ‘(ğ‘¥). $   

- Communication channel (ğ’³, ğ‘(ğ‘¦|ğ‘¥) , ğ’´)   
- Jointly typical sequences   
- Error ê°€ ì •ë§ë¡œ ì‘ì€ê°€?   
- Source-Channel Separation Theorem   

### Differential Entropy

### Gaussian Channel

### Rate Distortion Theorem

### Network Information Theory



--- 

## 2 Graph Learning ê¸°ì´ˆ

### Graph Convolutional Network

### Graph Filtering (Graph Signal Processing)    

--- 

### Graph Matching

### Graph Matching & Clustering




ê¸°ì´ˆ ì¤‘ ê¸°ì´ˆì¸ë°ë„ ë„ˆë¬´ ì–´ë µë‹¤ã…œã…œã…œ   

## Reference
lecture : ì •ë³´ ë° í•™ìŠµì´ë¡  ë‹¨ê¸°ê°•ì¢Œ     

web : 
- ìœ„í‚¤í”¼ë””ì•„ (https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%9D%B4%EB%A1%A0)    
- ë¸”ë¡œê·¸ (https://gem763.github.io/machine%20learning/%EC%A0%95%EB%B3%B4%EC%9D%B4%EB%A1%A0%EC%9D%98-%EA%B8%B0%EC%B4%88.html)   
paper : 
- []()
- 

