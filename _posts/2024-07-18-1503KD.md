---
permalink: /papers/model_compression/
title: "(1503) Distilling the Knowledge in a NN 논문 리뷰"
categories:
  - model_compression
tags:
  - paper
toc: true
toc_sticky: true
toc_label: "Paper Review"
date: 2024-07-18 15:30:00 +0900
last_modified_at: 2024-07-18 15:30:00 +0900
author_profile: false
header:
  overlay_image: assets\images\pages\2024-07-18KD.png
  overlay_filter: 0.5 
  teaser: assets\images\pages\2024-07-18KD.png
use_math: true
---
## Abstract
- MNIST에서 놀라운 결과들을 얻었고 an ensemble of models의 **"knowledge"**을 단일 모델로 **"distilling"**하여 상용 시스템의 음향 모델을 향상시켰다.
- 전체 모델(full models)이 혼동하는 세분화된(fine-grained) 클래스들을 구별하도록 학습하는 하나 이상의 전체 모델들 또는 전문적 모델들로 구성된 새로운 타입의 앙상블을 소개한다.

## 1. Introduction
- A conceptual block : 우리는 훈련된(trained) 모델의 지식을 학습된(learned) 파라미터 값과 동일시하는 경향이 있는데, 이는 동일한 knowledge는 유지하면서 모델의 형태(form)를 변경하는 것을 어렵게 만든다.
- **Knowledge** : 입력 벡터에서 출력 벡터로 학습된 매핑

<!-- 복잡한 모델은 많은 클래스를 구분하는 법을 학습하며, 주로 올바른 답의 평균 로그 확률을 최대화하는 것을 목표로 합니다.
학습의 부수적인 효과로, 모델은 모든 틀린 답에도 확률을 할당합니다. 이 확률은 매우 작을 수 있지만, 다른 틀린 답에 비해 상대적으로 큰 경우가 많습니다. 이러한 상대적 확률은 모델의 일반화 경향을 잘 보여줍니다. -->
<!-- 위 내용이 무슨 의미를 갖지? -->

문제 : 일반화를 잘하도록 모델을 훈련시키는 것이 좋으나, 일반적으로 올바른 일반화 방법에 대한 정보는 제공되지 않는다.

방법 : "distilling the knowledge from a large model into a small one" = "transfer generalization ability of the cumbersome model to small model"
  - 복잡한 모델이 만든 클래스 확률 값을 작은 모델의 훈련을 위한 "Soft Target"으로 사용한다.   
  (만약 복잡한 모델이 단순 모델들의 큰 앙상블이라면, Soft Targets으로 개별 예측 분포의 산술적/기하적 평균을 사용할 수 있다.)     
&nbsp; Q0. Soft Targets과 Hard Targets이 정확히 뭐지? 위 내용에 따르면 Hard Targets는 작은 모델이 만든 클래스 확률 값인가?   
&nbsp; Q1. Soft Targets은 High Entropy를 가지는 상황이 뭘까?   
&nbsp; Q2. Soft Targets은 왜 Hard Targets보다 training case 간 gradient에서 적은 분산을 가지나?   
  - **distillation** : 복잡한 모델이 타겟들의 Soft Set을 적합하게 만들어 낼 때 까지 Final Softmax의 온도(temperature)를 올리는 것   
  (Final Softmax로의 입력 값 = Logit 값)      
  - Objective Function에 **small term**을 추가하는 것이 작은 모델이 복잡한 모델이 제공하는 Soft Targets에 Matching하는 것 뿐만 아니라 True Targets을 예측하도록 도움을 준다.   
&nbsp; Q4. small term이 무엇일까? 작은 모델이 Soft Targets를 일반적으로 정확하게 일치(match)시킬 수 없다는 근거가 뭘까?   
&nbsp; Q5. 정답의 방향으로 오류를 발생시키는 것이 도움이 된다는데 어떤 방법으로? Outlier같은 에러의 방향을 정답으로 옮길 수 있나?   

## 2. Methods
### Distillation
\[
q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
\]

## 3. Conclusion

