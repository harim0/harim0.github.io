---
permalink: /papers/model_compression/
title: "(1503) Distilling the Knowledge in a NN 논문 공부"
categories:
  - model_compression
tags:
  - study
toc: true
toc_sticky: true
toc_label: "Paper Study"
date: 2024-07-18 15:30:00 +0900
last_modified_at: 2024-07-18 15:30:00 +0900
author_profile: false
header:
  overlay_image: assets\images\pages\2024-07-18KD.png
  overlay_filter: 0.5 
  teaser: assets\images\pages\2024-07-18KD.png
use_math: true
---


## Abstract
- MNIST에서 놀라운 결과들을 얻었고 an ensemble of models의 **"knowledge"**을 단일 모델로 **"distilling"**하여 상용 시스템의 음향 모델을 향상시켰다.
- 전체 모델(full models)이 혼동하는 세분화된(fine-grained) 클래스들을 구별하도록 학습하는 하나 이상의 전체 모델들 또는 전문적 모델들로 구성된 새로운 타입의 앙상블을 소개한다.


## 1. Introduction
- A conceptual block : 우리는 훈련된(trained) 모델의 지식을 학습된(learned) 파라미터 값과 동일시하는 경향이 있는데, 이는 동일한 knowledge는 유지하면서 모델의 형태(form)를 변경하는 것을 어렵게 만든다.
- **Knowledge** : Input 벡터에서 Output 벡터로 학습된 매핑

<!-- 복잡한 모델은 많은 클래스를 구분하는 법을 학습하며, 주로 올바른 답의 평균 로그 확률을 최대화하는 것을 목표로 합니다.
학습의 부수적인 효과로, 모델은 모든 틀린 답에도 확률을 할당합니다. 이 확률은 매우 작을 수 있지만, 다른 틀린 답에 비해 상대적으로 큰 경우가 많습니다. 이러한 상대적 확률은 모델의 일반화 경향을 잘 보여줍니다. -->
<!-- 위 내용이 무슨 의미를 갖지? -->

문제 : 일반화를 잘하도록 모델을 훈련시키는 것이 좋으나, 일반적으로 올바른 일반화 방법에 대한 정보는 제공되지 않는다.

방법 : "distilling the knowledge from a large model into a small one" = "transfer generalization ability of the cumbersome model to small model"
  - 복잡한 모델이 만든 클래스 확률 값을 작은 모델의 훈련을 위한 "Soft Target"으로 사용한다.   
  (만약 복잡한 모델이 단순 모델들의 큰 앙상블이라면, Soft Targets으로 개별 예측 분포의 산술적/기하적 평균을 사용할 수 있다.)  

&nbsp; Q0. Soft Targets과 Hard Targets이 정확히 뭐지? 위 내용에 따르면 Hard Targets는 작은 모델이 만든 클래스 확률 값인가?   
>  Soft Targets는 큰 모델이 소프트맥스 출력을 통해 생성한 클래스 확률 분포이고, Hard Targets는 데이터셋의 정답 레이블이다.   

&nbsp; Q1. Soft Targets은 High Entropy를 가지는 상황이 뭘까?   
>  Soft Targets이 High Entropy를 가지는 상황은 각 클래스에 대해 예측 확률이 고르게 분포하는 상황이다. 모델이 특정 클래스에 대해 확신하지 않고, 여러 클래스에 대해 비교적 높은 확률을 할당할 때 나타난다.   

&nbsp; Q2. Soft Targets은 왜 Hard Targets보다 training case 간 gradient에서 적은 분산을 가지나?   
>  Soft Targets은 각 클래스에 대한 확률 분포를 제공하므로, 모델이 학습하는 동안 각 클래스의 유사성 정보를 더 잘 반영할 수 있다. 이는 모델이 각 데이터 포인트에 대해 더 일관된 그래디언트를 생성하게 하여, 결과적으로 training case 간 gradient의 분산이 줄어든다. 반면, Hard Targets은 단일 클래스에만 초점을 맞추기 때문에 그래디언트의 변동성이 더 크다   

  - **distillation** : 복잡한 모델이 타겟들의 Soft Set을 적합하게 산출(produce)해낼 때 까지 Final Softmax의 온도(temperature)를 올리는 것   

  Caruana and his collaborators는 Softmax가 산출하는 확률 대신 Final Softmax의 입력 값(= Logit 값)을 작은 모델을 학습시키기위한 Targets으로 하여 복잡한 모델이 산출한 Logits과 작은 모델이 산출한 Logits간의 차이의 제곱을 최소화한다.      

  &rarr; 하지만 본 논문에서 제안하는 방법은 more general solution.     
  > Softmax 함수의 입력 값(Logits) vs. 출력 값(Soft Targets)   
  Logits는 모델의 마지막 Linear Layer(또는 Affine Layer)에서 나오는 값으로 확률이 아니기에 크기나 부호에 제한이 없는 값, 즉 각 클래스에 대해 모델이 얼마나 Confidence한지 나타내는 원시 점수이다. 반면, Soft Targets는 NonLinear 함수 Softmax를 Logits에 적용한 후의 값으로, 각 클래스에 대해 모델이 예측한 확률이다.   따라서, 원시 점수와 달리 확률 분포로 인해 클래스 간의 미세한 확률 차이를 반영하여 일반화 능력을 향상시킨다는 점에서 more general solution이다.       


  - Objective Function에 **small term**을 추가하는 것이 작은 모델이 복잡한 모델이 제공하는 Soft Targets에 Matching하는 것 뿐만 아니라 True Targets을 예측하도록 도움을 준다.   

&nbsp; Q4. small term이 무엇일까? 작은 모델이 Soft Targets를 일반적으로 정확하게 일치(match)시킬 수 없다는 근거가 뭘까?   
> small term은 모델이 정답 레이블을 예측하도록 돕는 항ㅎ. 작은 모델이 Soft Targets를 정확하게 일치시키지 못하는 이유는 큰 모델이 더 많은 파라미터와 복잡성을 가지고 있어 더 정교한 확률 분포를 생성할 수 있기 때문에 작은 모델은 이러한 복잡성을 충분히 표현하지 못할 수 있다. (?)      

&nbsp; Q5. 정답의 방향으로 오류를 발생시키는 것이 도움이 된다는데 어떤 방법으로? Outlier같은 에러의 방향을 정답으로 옮길 수 있나?     
> 모델이 정답 레이블에 더 가까운 예측을 하도록 유도하여 작은 모델이 Soft Targets뿐만 아니라 Hard Targets도 예측하도록 하는 것. 방법은 여러가지가 있겠지만 여기서는 Soft Targets와 Hard Targets를 혼합한 손실 함수를 사용하는 것을 다룬다.


## 2. Methods
### Distillation   

$q_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$    

Neural NetworkS는 일반적으로 각 클래스에 대해 계산된 Logit $z_i$를 다른 Logits과 비교하여 확률 $q_i$로 변환하는 Softmax Output Layer를 사용하여 클래스 확률 값을 산출한다.   
T는 기본적으로 1로 설정되는 온도로, 값이 커질수록 클래스에 대한 Softer 확률 분포를 산출한다.

&nbsp; Q6. Temperature이 정확히 뭔지 감이 오지 않아. Softmax함수와 유일하게 다른 부분이 T인데. 복잡한 모델이 자체 Softmax에서 높은 Temperature로 산출한 Transfer Set의 매 케이스에 대해 Soft Target 분포를 사용하여 Knowledge를 distilled model로 전이(transfer)한다. OK. 그리고 distilled model을 훈련시킬 때에 동일한 Temperature가 사용된다. 이것도 OK. 근데 왜 한번 훈련되고 난 이후로는 1의 Temperature를 사용한다는 거야? 무엇을 의미하길래?   
> Temperature(T)는 소프트맥스 함수의 출력을 부드럽게 만들기 위해 사용되는 매개변수이다.  높은 T는 훈련 중에만 사용되어 모델이 더 잘 일반화할 수 있도록 돕는 것이고, 훈련 후에 T를 1로 설정하는 이유는 모델이 실제 예측을 수행할 때는 일반적인 소프트맥스 출력을 사용하기 위해서다. (?)     

&nbsp; Q7. correct labels를 사용해서 어떻게 Soft Targets를 수정(modify)한다는 건가?   
> Soft Targets와 correct labels를 혼합한 새로운 타겟을 생성하거나 각 손실 함수의 가중치를 조정하여 Soft Targets와 Hard Targets의 기여도를 조절하는 방법으로 모델이 정답을 더 잘 예측하도록 수정할 수 있다. (?)    

2개의 Objective Functions의 Weighted Average &rarr; correct labels를 산출하도록 distilled model을 훈련시킬 수 있다.
1. Cross Entropy with the Soft Targets (distilled model의 Softmax와 동일한 T 사용)   
2. Cross Entropy with the correct labels (distilled model의 Softmax의 동일한 Logit, T=1)   

최고의 결과는 Object Function2에 낮은 weight를 가했을 때 얻어짐.   

&nbsp; Q8. 왜...?
> Object Function2에 낮은 weight를 가하는 것이 모델이 Hard Targets보다는 Soft Targets에 더 집중하도록 하여, 데이터 간의 유사성 정보를 더 잘 학습하게 한다.     

T가 변하더라도 Hard Target과 Soft Target의 상대적 기여도는 변하지 않았다.

&nbsp; Q9. 마찬가지로..왜...? 그리고 Soft Targets에서 얻어진 그래디언트의 크기가 왜 $1/T^2$로 스케일되는 것이지?
> Soft Targets에서 얻어진 그래디언트를 T^2로 곱해주기 때문 ^^. Soft Targets에서 얻어진 그래디언트의 크기가 $1/T^2$로 스케일되는 이유는, 높은 온도에서 소프트맥스 확률 분포가 더 부드러워지기 때문에 그래디언트의 크기가 줄어들기 때문이다.    


### Matching Logits
   
$
\frac{\partial C}{\partial z_i} = \frac{1}{T} (q_i - p_i) = \frac{1}{T} \left( \frac{e^{z_i/T}}{\sum_j e^{z_j/T}} - \frac{e^{v_i/T}}{\sum_j e^{v_j/T}} \right)
$   

만약 T가 Logits의 크기(magnitude)와 비교하여 크다면, $e^x \approx 1+x$를 사용하여 다음과 같이 approximate할 수 있다.   

$
\frac{\partial C}{\partial z_i} \approx \frac{1}{T} \left( \frac{1 + z_i / T}{N + \sum_j z_j / T} - \frac{1 + v_i / T}{N + \sum_j v_j / T} \right)   
$

만약 Logits이 매 transfer case에 대해 각각 zero-meaned이라면 다음과 같다.   

$
\frac{\partial C}{\partial z_i} \approx \frac{1}{T} \left( \frac{1 + z_i / T}{N} - \frac{1 + v_i / T}{N} \right)
$

$
\frac{\partial C}{\partial z_i} \approx \frac{1}{TN} \left( (1 + z_i / T) - (1 + v_i / T) \right)
$


$
\frac{\partial C}{\partial z_i} \approx \frac{1}{NT^2} (z_i - v_i)
$

결론적으로 T가 높다면, distillation은 $1/2(z_i-v_i)^2$를 최소화하는 것과 동일하다.   

낮은 T에서 distillation은 평균보다 negative한 Logits을 일치시키는데 관심이 적다. 이 Logit은 복잡한 모델을 훈련할 때 사용되는 비용 함수에 의해 거의 제약을 받지 않아 노이즈가 많을 수 있다. 하지만, 매우 부정적인 로짓은 복잡한 모델이 획득한 지식에 대해 유용한 정보를 전달할 수도 있다. 둘 중 어느 것이 맞는지는 경험적 질문이다. 여기서는 distilled model이 매우 작을 때 intermediate T가 best임을 발견했는데, 이는 큰 negative 로짓에 대해서는 무시하는 것이 도움이 된다는 점을 강력히 시사한다...   

&nbsp; Q10. 왜...?
> 복잡한 모델을 훈련할 때 사용하는 비용 함수(주로 교차 엔트로피)는 올바른 클래스의 확률을 최대화하는 데 중점을 두며, 잘못된 클래스에 매우 작은 확률을 부여하는 것에 대해 크게 상관하지 않는다.      

> 매우 부정적인 로짓은 모델이 특정 클래스에 대해 매우 확신을 갖고 있지 않다는 것을 나타낼 수 있으며, 이는 모델이 학습한 데이터의 구조와 분포에 대한 중요한 정보를 포함할 수 있다. 예를 들어, 어떤 2가 3으로 보일 가능성이 거의 없다는 정보는 데이터의 유사성 구조를 정의하는 데 유용할 수 있다.       

> 낮은 온도에서는 매우 부정적인 로짓이 거의 무시되지만, 이는 유용한 정보를 놓칠 수 있다. 반면, 높은 온도에서는 모든 로짓이 너무 평평해져서 중요한 정보가 희석될 수 있다. 따라서 중간 온도에서는 두 극단 사이 균형을 맞춰 복잡한 모델의 중요한 knowledge를 최대한 많이 학습할 수 있게 한다.       


## 3. Conclusion

### MNIST

### speech recognition

- 10개의 개별 모델 학습시켜 $P(ht|st; θ)$ 예측.    
(각 모델은 서로 다른 초기 매개변수 값으로 무작위 초기화 &rarr; 훈련된 모델에 충분한 다양성 부여 + 앙상블의 평균 예측이 개별 모델을 크게 능가함을 확인).   

### ensembles of specialists on very big datasets   

### Soft Targets as Regularizers


------   

# Reference   

blog : https://baeseongsu.github.io/posts/knowledge-distillation/

paper : [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

      