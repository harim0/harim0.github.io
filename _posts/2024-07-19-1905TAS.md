---
title: "(1905) Network Pruning via Transformable Architecture Search ë…¼ë¬¸ ê³µë¶€"
categories:
  - nas
tags:
  - study
toc: true
toc_sticky: true
toc_label: "Paper Study"
date: 2024-07-19 17:30:00 +0900
author_profile: false
header:
  overlay_image: assets/images/pages/2024-07-19.png
  overlay_filter: 0.5 
  teaser: assets/images/pages/2024-07-19.png
use_math: true
---

## Abstract    
**Pruned Networkì˜ êµ¬ì¡°ì (structure) í•œê³„**: Pruned Networkì˜ í­(width)ê³¼ ê¹Šì´(depth)ë¥¼ ë¯¸ë¦¬ ì •ì˜í•œ ë’¤ì— Unpruned Networkë¡œë¶€í„° íŒŒë¼ë¯¸í„°ë“¤ì„ Pruned Networkë¡œ ì „ì´(transfer)í•´ì•¼ í•œë‹¤.    

ì œì•ˆ 1: **Neural Architecture Search**ë¥¼ ì ìš©í•˜ì—¬ ìœ ì—°í•œ(flexible) ì±„ë„ê³¼ ë ˆì´ì–´ í¬ê¸°ë¥¼ ê°€ì§„ ë„¤íŠ¸ì›Œí¬ë¥¼ ì§ì ‘ ì°¾ëŠ”ë‹¤.    

- ì±„ë„/ë ˆì´ì–´ ìˆ˜ëŠ” Pruned Networkì˜ Lossë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµëœë‹¤. 
- LossëŠ” ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜(weights) ë¿ë§Œ ì•„ë‹ˆë¼ parameterized distributionì—ë„ backpropagateë˜ì–´ ì±„ë„/ë ˆì´ì–´ í¬ê¸°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¡°ì •í•  ìˆ˜ ìˆë‹¤.   
- <U>Pruned Networkì˜ Feature Map(A)</U>ì€ ë¶„í¬ì— ë”°ë¼ ìƒ˜í”Œë§ëœ(í¬ê¸°ê°€ ë‹¤ë¥¸ Kê°œì˜ ë„¤íŠ¸ì›Œí¬ì—ì„œ ìƒì„±ëœ) Kê°œì˜ Feature Map Fragmentsì˜ ì§‘í•©(aggregation)ì´ë‹¤.    

ì œì•ˆ 2: **Channel-wise Interpolation**ì„ ì ìš©í•˜ì—¬ Aggregation Procedureì—ì„œ ì±„ë„ í¬ê¸°ê°€ ë‹¤ë¥¸ <U>Feature Map(A)</U>ì˜ ì •ë ¬ì„ ìœ ì§€í•œë‹¤.        

- ë§¤ ë¶„í¬ë§ˆë‹¤ì˜ í¬ê¸°ì— ëŒ€í•œ ìµœëŒ€ í™•ë¥ ì€ Pruned Networkì˜ í­ê³¼ ê¹Šì´ë¡œ ì‚¬ìš©ë˜ë©°, ì´ íŒŒë¼ë¯¸í„°ëŠ” Knowledge Distillationê³¼ ê°™ì€ ì§€ì‹ ì „ì´ë¥¼ í†µí•´ í•™ìŠµëœë‹¤.    

&nbsp; Q1. parameterized distribution?    
&nbsp; Q2. Channel-wise Interpolation?    


## 1. Introduction   
![image](/assets/images/pages/240719TAS1.png){: width="400" height="400"}   
Fig. 1(a)ëŠ” ê¸°ì¡´ ë„¤íŠ¸ì›Œí¬ì˜ ì¤‘ë³µëœ(redundant) í•„í„°ë“¤ì„ ì œê±°í•˜ê³  ì˜ë¦°(slashed) ë„¤íŠ¸ì›Œí¬ë¥¼ fine-tuningí•œë‹¤. í•„í„°ì˜ importanceë¥¼ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œëŠ” í•„í„°ì˜ L2-norm, reconstruction error, í•™ìŠµê°€ëŠ¥í•œ scaling factor ë“±ì´ ìˆë‹¤.      
*"The accuracy of the pruned network is upper bounded by the hand-crafted structures or rules for structures."*     

&nbsp; Q3. L2-norm, reconstruction error?   
&nbsp; Q4. ì–´ë–»ê²Œ êµ¬ì¡°ë‚˜ ê·œì¹™ì— ë”°ë¥¸ Pruned Network ì •í™•ë„ì˜ ìƒí•œì„ ì„ êµ¬í• ê¹Œ?     

ì¼ë°˜ì  NAS ë°©ë²•ë“¤ì€ Network Topologyë¥¼ ìµœì í™”í•œë‹¤ë©´, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Network Sizeë¥¼ ìë™í™”í•˜ëŠ”ë° ì¤‘ì ì„ ë‘”ë‹¤.      
&rarr; **Transformable Architecture Search(TAS)**ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ í­ê³¼ ê¹Šì´ë¥¼ íš¨ê³¼ì (effectively)ì´ê³  íš¨ìœ¨ì (efficiently)ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆëŠ” ë¯¸ë¶„ê°€ëŠ¥í•œ ê²€ìƒ‰(search) ì•Œê³ ë¦¬ì¦˜ì´ë‹¤.    
- ë‹¤ë¥¸ ì±„ë„/ë ˆì´ì–´ í›„ë³´ë“¤ì— í•™ìŠµê°€ëŠ¥í•œ í™•ë¥ ì„ ë¶€ì—¬í•œë‹¤. í™•ë¥ ë¶„í¬ëŠ” Pruned Networksì— ì˜í•´ ìƒì„±ëœ Lossë¥¼ back-propagateí•˜ë©´ì„œ í•™ìŠµëœë‹¤.
- <U>Pruned Networks Feature Map(A)</U>ì˜ ë‹¤ë¥¸ ì±„ë„ í¬ê¸°ëŠ” **channel-wise interpolation**ì˜ ë„ì›€ìœ¼ë¡œ ì§‘ê³„(aggregate)ëœë‹¤.
- ë§¤ ë¶„í¬ ë³„ í¬ê¸°ì— ëŒ€í•œ ìµœëŒ€ í™•ë¥ ì€ Pruned Networkì˜ í­ê³¼ ê¹Šì´ë¡œ ì‚¬ìš©ëœë‹¤.       

&nbsp; Q5. Network Topology?    
Cf. Related Studies : Network Pruning, Unstructured Pruning Methods, Structured Pruning Methods, Criteria for Information Filters, Network Transformation, Knowledge Transfer    


## 2. Methods
1. í‘œì¤€ Classification í›ˆë ¨ ê³¼ì •ì— ë”°ë¼ Unpruned Large Networkë¥¼ í›ˆë ¨ì‹œí‚¨ë‹¤.   
2. ì œì•ˆëœ TASë¡œ Small Networkì˜ ê¹Šì´ì™€ í­ì„ ê²€ìƒ‰í•œë‹¤.   
3. ë‹¨ìˆœí•œ KD ì ‘ê·¼ìœ¼ë¡œ Unpruned Large Networkì—ì„œ Searched Small Networkë¡œ ì§€ì‹ì„ ì „ì´í•œë‹¤.   

### Transformable Architecture Search(TAS)   
    

$
O_j = \sum_{k=1}^{c_{in}} X_{k,:,:} \ast W_{j,k,:,:} \quad \text{where } 1 \leq j \leq c_{out} \tag{1}
$   
Output Feature Tensor($O_j$)ëŠ” lë²ˆì§¸ convolutional layerì—ì„œ Input Feature Tensor($X$)ì™€ Convolutional Kernel Weight($W$)ì˜ convolution ì—°ì‚°ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤. Channel Pruningìœ¼ë¡œ ouput channel($c_{out}$)ì˜ ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë©°, ì—°ì†ì ìœ¼ë¡œ ë‹¤ìŒ ë ˆì´ì–´ì˜ input channel($c_{in}$) ìˆ˜ ë˜í•œ ì¤„ì¼ìˆ˜ ìˆë‹¤.   

**Search for Width**   
$
p_j = \frac{\exp(\alpha_j)}{\sum_{k=1}^{|C|} \exp(\alpha_k)} \quad \text{where } 1 \leq j \leq |C| \tag{2}
$   
í•œ ë ˆì´ì–´ì—ì„œ ê°€ëŠ¥í•œ ì±„ë„ ìˆ˜ì˜ ë¶„í¬($\alpha$)ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ì‚¬ìš©í•˜ì—¬ ì±„ë„ ìˆ˜ì˜ jë²ˆì§¸ í›„ë³´ë¥¼ ê³ ë¥´ëŠ” í™•ë¥ ì´ë‹¤.     
  
&nbsp; Q6. ì™œ ìœ„ì˜ ê³¼ì •ì—ì„œ sampling ì—°ì‚°ì´ ë¯¸ë¶„ë¶ˆê°€ëŠ¥í•˜ì—¬ $p_j$ì—ì„œ $\alpha_j$ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ back-propagatingí•˜ëŠ” ê²ƒì„ ë°©í•´í•˜ëŠ”ê°€?    
> sampling ì—°ì‚°ì—ì„œ í•˜ë‚˜ì˜ ì´ì‚° ê°’ì„ ê³ ë¥´ê¸° ë•Œë¬¸ì— non-smooth transitionìœ¼ë¡œ ì¸í•˜ì—¬ ë¯¸ë¶„ë¶ˆê°€ëŠ¥í•˜ë‹¤.      

$
\hat{p_j} = \frac{\exp((\log(p_j) + o_j)/\tau)}{\sum_{k=1}^{|C|} \exp((\log(p_k) + o_k)/\tau)} \quad \text{s.t. } o_j = -\log(-\log(u)) \& u \sim U(0,1) \tag{3}
$   
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Gumbel-Softmax(3)ìœ¼ë¡œ sampling ê³¼ì •ì„ ì—°ì†ì ìœ¼ë¡œ ê·¼ì‚¬í™”í•´ì„œ(=ë¶€ë“œëŸ½ê²Œ) $\alpha$ë¥¼ ìµœì í™”í•œë‹¤.    

> ì´ì‚° ìƒ˜í”Œë§ì˜ ì—°ì†ì ì¸ ê·¼ì‚¬ê°’ì„ ë§Œë“¤ê¸° ìœ„í•´ ì„ íƒì˜ ëœë¤ì„±ì„ ì£¼ëŠ” Gumbel Noise($o$)ë¥¼ ë”í•´ì£¼ì—ˆê³ , ì´ëŠ” í™•ë¥ ì  ìƒ˜í”Œë§ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ê³  gradient ê¸°ë°˜ ìµœì í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.    

&nbsp; Q7. ì™œ $\tau$ &rarr; 0ì´ë©´ $\hat{p}$ê°€ one-shotì´ ë˜ê³ , Gumbel-Softmax ë¶„í¬ëŠ” ë²”ì£¼í˜•(categorical) ë¶„í¬ì™€ ë™ì¼í•´ì§€ë‚˜?     
> ì˜¨ë„ê°€ ë‚®ì•„ì§€ë©´, Softmax í•¨ìˆ˜ëŠ” ì…ë ¥ê°’ ê°„ì˜ ì°¨ì´ë¥¼ ë” ê·¹ë‹¨ì ìœ¼ë¡œ ë°˜ì˜í•˜ê²Œ ë˜ì–´, ê°€ì¥ í° ê°’ì„ ê°€ì§„ í•­ëª©ì´ 1ì— ê°€ê¹Œì›Œì§€ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì— ê°€ê¹Œì›Œì ¸ <U>íŠ¹ì •í•œ ì‹œì ì— í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¥¼ ì„ íƒ</U>(=one-shot)í•˜ê²Œ ë˜ë¯€ë¡œ ë²”ì£¼í˜• ë¶„í¬ì™€ ë™ì¼í•´ì§„ë‹¤.(?)     

$
\hat{O} = \sum_{j \in I} \frac{\exp((\log(p_j) + o_j)/\tau)}{\sum_{k \in I} \exp((\log(p_k) + o_k)/\tau)} \times \text{CWI}(O_{1:C_j,:,:}, \max(C_I)) \quad \text{s.t. } I \sim T_{\hat(p)} \tag{4}
$   
ì œì•ˆ ë°©ë²•ì˜ Feature Map($\hat{O}$)ì€ í¬ê¸°ê°€ ë‹¤ë¥¸ ì›ë³¸ Feature Map Fragmentì˜ ê°€ì¤‘ì¹˜($\hat{p}$)ë“¤ì˜ í•©ì´ë‹¤.    
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì¶”ê°€ì  íŒŒë¼ë¯¸í„°ì™€ ë¹„ìš©ì´ ë“¤ì§€ ì•ŠëŠ” 3D adaptive average pooling operationì„ $CWI$ë¡œ ì„ íƒí–ˆë‹¤. ë‹¤ë¥¸ í¬ê¸°ì˜ Feature Mapë“¤ì€ Weighted Sum ì—°ì‚°ì„ ìœ„í•´ Channel Wise Interpolation($CWI$)ì„ í†µí•´ ì •ë ¬ëœë‹¤.    
<!-- $\hat{p}$ë¡œ ë§¤ê°œë³€ìˆ˜í™”ëœ ë‹¤í•­ í™•ë¥  ë¶„í¬ $T_{\hat{p}}$ì—ì„œ ë©”ëª¨ë¦¬ ë¹„ìš©ì„ ì¤„ì´ê³ ì í›„ë³´ë“¤ ì¤‘ ì¸ë±ìŠ¤ Iì˜ small subsetì„ ì„ íƒí•˜ì—¬ BNìœ¼ë¡œ ì •ê·œí™”í•˜ê³  CWIë¥¼ ì ìš©í•œë‹¤. -->

> - **CWIë¥¼ í†µí•œ í¬ê¸° ì •ë ¬**      
$B = \text{CWI}(A, C_{\text{out}})$ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€$B \in \mathbb{R}^{C_{\text{out}} \times H \times W}$, $A \in \mathbb{R}^{C_{\text{in}} \times H \times W}$      
$B_{i,h,w} = \text{mean}(A_{s:e-1,h,w})$ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€$s = \left\lfloor \frac{i \times C_{\text{in}}}{C_{\text{out}}} \right\rfloor$, $e = \left\lceil \frac{(i+1) \times C_{\text{in}}}{C_{\text{out}}} \right\rceil$   
*B: aligned output feature map, ğ´: input feature map    
CWIëŠ” ğ´ì˜ ì±„ë„ì„ $C_{\text{out}}$ì±„ë„ë¡œ ì¶•ì†Œí•˜ê¸° ìœ„í•´ ê° ì±„ë„ì˜ í‰ê· ì„ ê³„ì‚°í•œë‹¤.     

uniform ë¶„í¬ë¥¼ í†µí•œ ìƒ˜í”Œë§ê³¼ ë¹„êµí–ˆì„ ë•Œ, ì ìš©ëœ í™•ë¥  ê¸°ë°˜ ìƒ˜í”Œë§ì€ ì—¬ëŸ¬ ë°˜ë³µ í›„ per-iterationìœ¼ë¡œ ì¸í•œ gradients ì°¨ì´ë¥¼ ì•½í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤.        
   
> 
    

**Search for Depth**    
$O_{out} = \sum_{l=1}^L \hat{q}_l \times \text{CWI}(\hat{O}_l, C_{out}) \tag{5}$   


**Searching Objectives**    
$
\min_{A} \mathcal{L}_{\text{val}}(\omega^*_A, A) \quad \text{s.t. } \omega^*_A = \arg\min_{\omega} \mathcal{L}_{\text{train}}(\omega, A) \tag{6}
$    

$
\mathcal{L}_{\text{val}} = - \log\left( \frac{\exp(z_y)}{\sum_{j=1}^{|z|} \exp(z_j)} \right) + \lambda \mathcal{L}_{\text{cost}} \tag{7}
$    

$
\mathcal{L}_{\text{cost}} = 
\begin{cases}
\log(E_{\text{cost}}(A)) & \text{if } F_{\text{cost}}(A) > (1 + t) \times R
0 & \text{if } (1 - t) \times R < F_{\text{cost}}(A) < (1 + t) \times R - \log(E_{\text{cost}}(A)) & \text{if } F_{\text{cost}}(A) < (1 - t) \times R
\end{cases} \tag{8}
$    


### Knowledge Transfer

$
\mathcal{L}_{\text{match}} = - \sum_{i=1}^{|z|} \frac{\exp(\hat{z}_i/T)}{\sum_{j=1}^{|z|} \exp(\hat{z}_j/T)} \log\left( \frac{\exp(z_i/T)}{\sum_{j=1}^{|z|} \exp(z_j/T)} \right) \tag{9}
$    

$
\mathcal{L}_{\text{KD}} = - \lambda \log\left( \frac{\exp(z_y)}{\sum_{j=1}^{|z|} \exp(z_j)} \right) + (1 - \lambda) \mathcal{L}_{\text{match}} \quad \text{s.t. } 0 \leq \lambda \leq 1 \tag{10}
$    




## 3. Conclusion





## 4. Reference
blog :     

paper : [Network Pruning via Transformable Architecture Search](https://arxiv.org/abs/1905.09717)

