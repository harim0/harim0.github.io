---
title: "(1905) Network Pruning via Transformable Architecture Search 논문 공부"
categories:
  - nas
tags:
  - study
toc: true
toc_sticky: true
toc_label: "Paper Study"
date: 2024-07-19 17:30:00 +0900
author_profile: false
header:
  overlay_image: assets/images/pages/2024-07-19.png
  overlay_filter: 0.5 
  teaser: assets/images/pages/2024-07-19.png
use_math: true
---

## Abstract    
**Pruned Network의 구조적(structure) 한계**: Pruned Network의 폭(width)과 깊이(depth)를 미리 정의한 뒤에 Unpruned Network로부터 파라미터들을 Pruned Network로 전이(transfer)해야 한다.    

제안 1: **Neural Architecture Search**를 적용하여 유연한(flexible) 채널과 레이어 크기를 가진 네트워크를 직접 찾는다.    

- 채널/레이어 수는 Pruned Network의 Loss를 최소화하도록 학습된다. 
- Loss는 네트워크 가중치(weights) 뿐만 아니라 parameterized distribution에도 backpropagate되어 채널/레이어 크기를 명시적으로 조정할 수 있다.   
- <U>Pruned Network의 Feature Map(A)</U>은 분포에 따라 샘플링된(크기가 다른 K개의 네트워크에서 생성된) K개의 Feature Map Fragments의 집합(aggregation)이다.    

제안 2: **Channel-wise Interpolation**을 적용하여 Aggregation Procedure에서 채널 크기가 다른 <U>Feature Map(A)</U>의 정렬을 유지한다.        

- 매 분포마다의 크기에 대한 최대 확률은 Pruned Network의 폭과 깊이로 사용되며, 이 파라미터는 Knowledge Distillation과 같은 지식 전이를 통해 학습된다.    

&nbsp; Q1. parameterized distribution?    
&nbsp; Q2. Channel-wise Interpolation?    


## 1. Introduction   
![image](/assets/images/pages/240719TAS1.png){: width="400" height="400"}   
Fig. 1(a)는 기존 네트워크의 중복된(redundant) 필터들을 제거하고 잘린(slashed) 네트워크를 fine-tuning한다. 필터의 importance를 판단하는 기준으로는 필터의 L2-norm, reconstruction error, 학습가능한 scaling factor 등이 있다.      
*"The accuracy of the pruned network is upper bounded by the hand-crafted structures or rules for structures."*     

&nbsp; Q3. L2-norm, reconstruction error?   
&nbsp; Q4. 어떻게 구조나 규칙에 따른 Pruned Network 정확도의 상한선을 구할까?     

일반적 NAS 방법들은 Network Topology를 최적화한다면, 본 논문에서는 Network Size를 자동화하는데 중점을 둔다.      
&rarr; **Transformable Architecture Search(TAS)**는 네트워크의 폭과 깊이를 효과적(effectively)이고 효율적(efficiently)으로 찾을 수 있는 미분가능한 검색(search) 알고리즘이다.    
- 다른 채널/레이어 후보들에 학습가능한 확률을 부여한다. 확률분포는 Pruned Networks에 의해 생성된 Loss를 back-propagate하면서 학습된다.
- <U>Pruned Networks Feature Map(A)</U>의 다른 채널 크기는 **channel-wise interpolation**의 도움으로 집계(aggregate)된다.
- 매 분포 별 크기에 대한 최대 확률은 Pruned Network의 폭과 깊이로 사용된다.       

&nbsp; Q5. Network Topology?    
Cf. Related Studies : Network Pruning, Unstructured Pruning Methods, Structured Pruning Methods, Criteria for Information Filters, Network Transformation, Knowledge Transfer    


## 2. Methods
1. 표준 Classification 훈련 과정에 따라 Unpruned Large Network를 훈련시킨다.   
2. 제안된 TAS로 Small Network의 깊이와 폭을 검색한다.   
3. 단순한 KD 접근으로 Unpruned Large Network에서 Searched Small Network로 지식을 전이한다.   

### Transformable Architecture Search(TAS)   
    

$$
\begin{align}
O_j = \sum_{k=1}^{c_{in}} X_{k,:,:} \ast W_{j,k,:,:} \quad \text{where } 1 \leq j \leq c_{out} \tag{1}
\end{align}
$$    
Output Feature Tensor($O_j$)는 l번째 convolutional layer에서 Input Feature Tensor($X$)와 Convolutional Kernel Weight($W$)의 convolution 연산으로 만들어진다. Channel Pruning으로 ouput channel($c_{out}$)의 수를 줄일 수 있으며, 연속적으로 다음 레이어의 input channel($c_{in}$) 수 또한 줄일수 있다.   

**Search for Width**   
$$
\begin{align}
p_j = \frac{\exp(\alpha_j)}{\sum_{k=1}^{|C|} \exp(\alpha_k)} \quad \text{where } 1 \leq j \leq |C| \tag{2}
\end{align}
$$   
한 레이어에서 가능한 채널 수의 분포($\alpha$)를 파라미터로 사용하여 채널 수의 j번째 후보를 고르는 확률이다.     
  
&nbsp; Q6. 왜 위의 과정에서 sampling 연산이 미분불가능하여 $p_j$에서 $\alpha_j$로 그래디언트를 back-propagating하는 것을 방해하는가?    
> sampling 연산에서 하나의 이산 값을 고르기 때문에 non-smooth transition으로 인하여 미분불가능하다.      

$$
\begin{align}
\hat{p_j} = \frac{\exp((\log(p_j) + o_j)/\tau)}{\sum_{k=1}^{|C|} \exp((\log(p_k) + o_k)/\tau)} \quad \text{s.t. } o_j = -\log(-\log(u)) \& u \sim U(0,1) \tag{3}
\end{align}
$$   
본 논문에서는 Gumbel-Softmax(3)으로 sampling 과정을 연속적으로 근사화해서(=부드럽게) $\alpha$를 최적화한다.    

> 이산 샘플링의 연속적인 근사값을 만들기 위해 선택의 랜덤성을 주는 Gumbel Noise($o$)를 더해주었고, 이는 확률적 샘플링을 가능하게 하고 gradient 기반 최적화를 가능하게 한다.    

&nbsp; Q7. 왜 $\tau$ &rarr; 0이면 $\hat{p}$가 one-shot이 되고, Gumbel-Softmax 분포는 범주형(categorical) 분포와 동일해지나?     
> 온도가 낮아지면, Softmax 함수는 입력값 간의 차이를 더 극단적으로 반영하게 되어, 가장 큰 값을 가진 항목이 1에 가까워지고 나머지는 0에 가까워져 <U>특정한 시점에 하나의 클래스를 선택</U>(=one-shot)하게 되므로 범주형 분포와 동일해진다.(?)     

$$
\begin{align}
\hat{O} = \sum_{j \in I} \frac{\exp((\log(p_j) + o_j)/\tau)}{\sum_{k \in I} \exp((\log(p_k) + o_k)/\tau)} \times \text{CWI}(O_{1:C_j,:,:}, \max(C_I)) \quad \text{s.t. } I \sim T_{\hat{p}} \tag{4}
\end{align}
$$   
제안 방법의 Feature Map($\hat{O}$)은 크기가 다른 원본 Feature Map Fragment의 가중치($\hat{p}$)들의 합이다.    
본 논문에서는 추가적 파라미터와 비용이 들지 않는 3D adaptive average pooling operation을 $CWI$로 선택했다. 다른 크기의 Feature Map들은 Weighted Sum 연산을 위해 Channel Wise Interpolation($CWI$)을 통해 정렬된다.    
<!-- $\hat{p}$로 매개변수화된 다항 확률 분포 $T_{\hat{p}}$에서 메모리 비용을 줄이고자 후보들 중 인덱스 I의 small subset을 선택하여 BN으로 정규화하고 CWI를 적용한다. -->

> - **CWI를 통한 크기 정렬**      
$B = \text{CWI}(A, C_{\text{out}})$　　　　　　　　$B \in \mathbb{R}^{C_{\text{out}} \times H \times W}$, $A \in \mathbb{R}^{C_{\text{in}} \times H \times W}$      
$B_{i,h,w} = \text{mean}(A_{s:e-1,h,w})$　　　　　　　　$s = \left\lfloor \frac{i \times C_{\text{in}}}{C_{\text{out}}} \right\rfloor$, $e = \left\lceil \frac{(i+1) \times C_{\text{in}}}{C_{\text{out}}} \right\rceil$   
*B: aligned output feature map, 𝐴: input feature map    
CWI는 𝐴의 채널을 $C_{\text{out}}$채널로 축소하기 위해 각 채널의 평균을 계산한다.     

uniform 분포 샘플링과 비교했을 때, 적용된 확률 기반 샘플링은 여러 반복 후 per-iteration으로 인한 gradients 차이를 약화시킬 수 있다.        
   
> Uniform 분포 샘플링은 특정 샘플이 반복적으로 선택될 가능성이 적다. 따라서 매 반복마다 샘플들이 다양하게 선택되면 모델이 모든 샘플에 대해 고르게 학습하는 것을 방해하여 gradients의 변화가 커진다. 반면, 확률 기반 샘플링은 각 샘플의 중요도나 확률을 반영하여 선택하므로 gradients의 변동을 줄이고, 모델의 안정적인 학습을 돕는다.    
    

**Search for Depth**   
$$
\begin{align}
O_{\text{out}} = \sum_{l=1}^{L} \hat{q}_l \times \text{CWI}(\hat{O}^l, C_{\text{out}}) \tag{5}
\end{align}
$$    
가능한 모든 깊이의 집합으로 Pruned Network의 Final Output Feature Map($O_{out}$)은 
L개의 convolutional layer가 있는 네트워크에서 가능한 레이어 수의 분포 $\beta$를 사용하여 깊이 l에 대한 샘플링 분포 $\hat{q}_l$를 통해 계산된다. 이는 마지막 classification layer에 공급되어 width 파라미터 α와 depth 파라미터 β에 대해 backpropagate할 수 있게 한다.    

**Searching Objectives**       

$$
\begin{align}
\min_{A} \mathcal{L}_{\text{val}}(\omega^*_A, A) \quad &\text{s.t. } \omega^*_A = \arg\min_{\omega} \mathcal{L}_{\text{train}}(\omega, A) \tag{6}
\end{align}
$$     

TAS의 목표는 train 데이터에 대한 CE loss인 $L_{\text{train}}$을 최소화하면서 Pruned Network의 가중치를 최적화하고, 학습 후 valid 데이터에 대한 최소 $L_{\text{val}}$을 갖는 최적화된 아키텍처 A를 찾는 것이다.    

$$
\mathcal{L}_{\text{val}} = - \log\left( \frac{\exp(z_y)}{\sum_{j=1}^{|z|} \exp(z_j)} \right) + \lambda_{\text{cost}} \mathcal{L}_{\text{cost}} \tag{7}
$$     

Softmax CE Loss에 Cost에 대한 penalty를 더해 $L_{\text{val}}$를 정의했다.     

$$
L_{\text{cost}} = 
\begin{cases} 
\log(E_{\text{cost}}(A)) & \text{if } F_{\text{cost}}(A) > (1 + t) \times R \\ 
0 & \text{if } (1 - t) \times R < F_{\text{cost}}(A) < (1 + t) \times R \\ 
-\log(E_{\text{cost}}(A)) & \text{if } F_{\text{cost}}(A) < (1 - t) \times R 
\end{cases} \tag{8}
$$     

아키텍처 𝐴의 실제 Cost($F_{\text{cost}}(A)$)와 목표 Cost(R)를 비교하여 $L_{\text{cost}}$를 정의하였다.      


### Knowledge Transfer

$$
\mathcal{L}_{\text{match}} = - \sum_{i=1}^{|z|} \frac{\exp(\hat{z}_i/T)}{\sum_{j=1}^{|z|} \exp(\hat{z}_j/T)} \log\left( \frac{\exp(z_i/T)}{\sum_{j=1}^{|z|} \exp(z_j/T)} \right) \tag{9}
$$       

small network의 예측값 z가 unpruned networK의 soft targets과 일치되도록 한다.     
$\hat{z}$은 pretrain된 unpruned network의 logit output vector이다.      


$$
\mathcal{L}_{\text{KD}} = - \lambda \log\left( \frac{\exp(z_y)}{\sum_{j=1}^{|z|} \exp(z_j)} \right) + (1 - \lambda) \mathcal{L}_{\text{match}} \quad \text{s.t. } 0 \leq \lambda \leq 1 \tag{10}
$$    

searched network를 얻은 후 먼저 Unpruned Network를 Pretrain하고, (10)에 의해 Unpruned Network에서 전이하여 searched network를 최적화한다.   

> - **Kullback-Leibler(KL) Divergence**   
$D_{KL}(P \parallel Q) = \sum_{i} P(i) \log \left( \frac{P(i)}{Q(i)} \right)$   
𝑃는 실제 분포, 𝑄는 예상 분포로 $D_{KL}(P \parallel Q)$는 두 확률 분포 𝑄와 𝑃의 차이를 나타낸다. 값이 작을수록 두 분포가 유사하다.    


## 3. Results   

![image](/assets/images/pages/20TAS_table1.png){: width="600" height="600"}   
![image](/assets/images/pages/20TAS_table2.png){: width="600" height="600"}     

## 4. Conclusion   
서로 다른 아키텍처를 비교하는 기존 NAS와 달리 구조가 동일한 네트워크 중 폭과 깊이가 작은 후보들을 최적화한다.   

1. 네트워크 최적의 width와 depth를 찾고자 NAS 적용   
2. KD 알고리즘 적용    

## 5. Reference

paper : [Network Pruning via Transformable Architecture Search](https://arxiv.org/abs/1905.09717)

