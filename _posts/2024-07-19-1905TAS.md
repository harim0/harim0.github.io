---
title: "(1905) Network Pruning via Transformable Architecture Search 논문 공부"
categories:
  - nas
tags:
  - study
toc: true
toc_sticky: true
toc_label: "Paper Study"
date: 2024-07-19 17:30:00 +0900
author_profile: false
header:
  overlay_image: assets/images/pages/2024-07-19.png
  overlay_filter: 0.5 
  teaser: assets/images/pages/2024-07-19.png
use_math: true
---

## Abstract    
**Pruned Network의 구조적(structure) 한계**: Pruned Network의 폭(width)과 깊이(depth)를 미리 정의한 뒤에 Unpruned Network로부터 파라미터들을 Pruned Network로 전이(transfer)해야 한다.    

제안 1: **Neural Architecture Search**를 적용하여 유연한(flexible) 채널과 레이어 크기를 가진 네트워크를 직접 찾는다.    

- 채널/레이어 수는 Pruned Network의 Loss를 최소화하도록 학습된다. 
- Loss는 네트워크 가중치(weights) 뿐만 아니라 parameterized distribution에도 backpropagate되어 채널/레이어 크기를 명시적으로 조정할 수 있다.   
- <U>Pruned Network의 Feature Map(A)</U>은 분포에 따라 샘플링된(크기가 다른 K개의 네트워크에서 생성된) K개의 Feature Map Fragments의 집합(aggregation)이다.    

제안 2: **Channel-wise Interpolation**을 적용하여 Aggregation Procedure에서 채널 크기가 다른 <U>Feature Map(A)</U>의 정렬을 유지한다.        

- 매 분포마다의 크기에 대한 최대 확률은 Pruned Network의 폭과 깊이로 사용되며, 이 파라미터는 Knowledge Distillation과 같은 지식 전이를 통해 학습된다.    

&nbsp; Q1. parameterized distribution?    
&nbsp; Q2. Channel-wise Interpolation?    


## 1. Introduction   
![image](/assets/images/pages/240719TAS1.png){: width="400" height="400"}   
Fig. 1(a)는 기존 네트워크의 중복된(redundant) 필터들을 제거하고 잘린(slashed) 네트워크를 fine-tuning한다. 필터의 importance를 판단하는 기준으로는 필터의 L2-norm, reconstruction error, 학습가능한 scaling factor 등이 있다.      
*"The accuracy of the pruned network is upper bounded by the hand-crafted structures or rules for structures."*     

&nbsp; Q3. L2-norm, reconstruction error?   
&nbsp; Q4. 어떻게 구조나 규칙에 따른 Pruned Network 정확도의 상한선을 구할까?     

일반적 NAS 방법들은 Network Topology를 최적화한다면, 본 논문에서는 Network Size를 자동화하는데 중점을 둔다.      
&rarr; **Transformable Architecture Search(TAS)**는 네트워크의 폭과 깊이를 효과적(effectively)이고 효율적(efficiently)으로 찾을 수 있는 미분가능한 검색(search) 알고리즘이다.    
- 다른 채널/레이어 후보들에 학습가능한 확률을 부여한다. 확률분포는 Pruned Networks에 의해 생성된 Loss를 back-propagate하면서 학습된다.
- <U>Pruned Networks Feature Map(A)</U>의 다른 채널 크기는 **channel-wise interpolation**의 도움으로 집계(aggregate)된다.
- 매 분포 별 크기에 대한 최대 확률은 Pruned Network의 폭과 깊이로 사용된다.       

&nbsp; Q5. Network Topology?    
Cf. Related Studies : Network Pruning, Unstructured Pruning Methods, Structured Pruning Methods, Criteria for Information Filters, Network Transformation, Knowledge Transfer    


## 2. Methods
1. 표준 Classification 훈련 과정에 따라 Unpruned Large Network를 훈련시킨다.   
2. 제안된 TAS로 Small Network의 깊이와 폭을 검색한다.   
3. 단순한 KD 접근으로 Unpruned Large Network에서 Searched Small Network로 지식을 전이한다.   

### Transformable Architecture Search(TAS)   
    

$
O_j = \sum_{k=1}^{c_{in}} X_{k,:,:} \ast W_{j,k,:,:} \quad \text{where } 1 \leq j \leq c_{out} \tag{1}
$   
Output Feature Tensor($O_j$)는 l번째 convolutional layer에서 Input Feature Tensor($X$)와 Convolutional Kernel Weight($W$)의 convolution 연산으로 만들어진다. Channel Pruning으로 ouput channel($c_{out}$)의 수를 줄일 수 있으며, 연속적으로 다음 레이어의 input channel($c_{in}$) 수 또한 줄일수 있다.   

**Search for Width**   
$
p_j = \frac{\exp(\alpha_j)}{\sum_{k=1}^{|C|} \exp(\alpha_k)} \quad \text{where } 1 \leq j \leq |C| \tag{2}
$   
한 레이어에서 가능한 채널 수의 분포($\alpha$)를 파라미터로 사용하여 채널 수의 j번째 후보를 고르는 확률이다.     
  
&nbsp; Q6. 왜 위의 과정에서 sampling 연산이 미분불가능하여 $p_j$에서 $\alpha_j$로 그래디언트를 back-propagating하는 것을 방해하는가?    
> sampling 연산에서 하나의 이산 값을 고르기 때문에 non-smooth transition으로 인하여 미분불가능하다.      

$
\hat{p_j} = \frac{\exp((\log(p_j) + o_j)/\tau)}{\sum_{k=1}^{|C|} \exp((\log(p_k) + o_k)/\tau)} \quad \text{s.t. } o_j = -\log(-\log(u)) \& u \sim U(0,1) \tag{3}
$   
본 논문에서는 Gumbel-Softmax(3)으로 sampling 과정을 연속적으로 근사화해서(=부드럽게) $\alpha$를 최적화한다.    

> 이산 샘플링의 연속적인 근사값을 만들기 위해 선택의 랜덤성을 주는 Gumbel Noise($o$)를 더해주었고, 이는 확률적 샘플링을 가능하게 하고 gradient 기반 최적화를 가능하게 한다.    

&nbsp; Q7. 왜 $\tau$ &rarr; 0이면 $\hat{p}$가 one-shot이 되고, Gumbel-Softmax 분포는 범주형(categorical) 분포와 동일해지나?     
> 온도가 낮아지면, Softmax 함수는 입력값 간의 차이를 더 극단적으로 반영하게 되어, 가장 큰 값을 가진 항목이 1에 가까워지고 나머지는 0에 가까워져 <U>특정한 시점에 하나의 클래스를 선택</U>(=one-shot)하게 되므로 범주형 분포와 동일해진다.(?)     

$
\hat{O} = \sum_{j \in I} \frac{\exp((\log(p_j) + o_j)/\tau)}{\sum_{k \in I} \exp((\log(p_k) + o_k)/\tau)} \times \text{CWI}(O_{1:C_j,:,:}, \max(C_I)) \quad \text{s.t. } I \sim T_{\hat(p)} \tag{4}
$   
제안 방법의 Feature Map($\hat{O}$)은 크기가 다른 원본 Feature Map Fragment의 가중치($\hat{p}$)들의 합이다.    
본 논문에서는 추가적 파라미터와 비용이 들지 않는 3D adaptive average pooling operation을 $CWI$로 선택했다. 다른 크기의 Feature Map들은 Weighted Sum 연산을 위해 Channel Wise Interpolation($CWI$)을 통해 정렬된다.    
<!-- $\hat{p}$로 매개변수화된 다항 확률 분포 $T_{\hat{p}}$에서 메모리 비용을 줄이고자 후보들 중 인덱스 I의 small subset을 선택하여 BN으로 정규화하고 CWI를 적용한다. -->

> - **CWI를 통한 크기 정렬**      
$B = \text{CWI}(A, C_{\text{out}})$　　　　　　　　$B \in \mathbb{R}^{C_{\text{out}} \times H \times W}$, $A \in \mathbb{R}^{C_{\text{in}} \times H \times W}$      
$B_{i,h,w} = \text{mean}(A_{s:e-1,h,w})$　　　　　　　　$s = \left\lfloor \frac{i \times C_{\text{in}}}{C_{\text{out}}} \right\rfloor$, $e = \left\lceil \frac{(i+1) \times C_{\text{in}}}{C_{\text{out}}} \right\rceil$   
*B: aligned output feature map, 𝐴: input feature map    
CWI는 𝐴의 채널을 $C_{\text{out}}$채널로 축소하기 위해 각 채널의 평균을 계산한다.     

uniform 분포를 통한 샘플링과 비교했을 때, 적용된 확률 기반 샘플링은 여러 반복 후 per-iteration으로 인한 gradients 차이를 약화시킬 수 있다.        
   
> 
    

**Search for Depth**    
$O_{out} = \sum_{l=1}^L \hat{q}_l \times \text{CWI}(\hat{O}_l, C_{out}) \tag{5}$   


**Searching Objectives**    
$
\min_{A} \mathcal{L}_{\text{val}}(\omega^*_A, A) \quad \text{s.t. } \omega^*_A = \arg\min_{\omega} \mathcal{L}_{\text{train}}(\omega, A) \tag{6}
$    

$
\mathcal{L}_{\text{val}} = - \log\left( \frac{\exp(z_y)}{\sum_{j=1}^{|z|} \exp(z_j)} \right) + \lambda \mathcal{L}_{\text{cost}} \tag{7}
$    

$
\mathcal{L}_{\text{cost}} = 
\begin{cases}
\log(E_{\text{cost}}(A)) & \text{if } F_{\text{cost}}(A) > (1 + t) \times R
0 & \text{if } (1 - t) \times R < F_{\text{cost}}(A) < (1 + t) \times R - \log(E_{\text{cost}}(A)) & \text{if } F_{\text{cost}}(A) < (1 - t) \times R
\end{cases} \tag{8}
$    


### Knowledge Transfer

$
\mathcal{L}_{\text{match}} = - \sum_{i=1}^{|z|} \frac{\exp(\hat{z}_i/T)}{\sum_{j=1}^{|z|} \exp(\hat{z}_j/T)} \log\left( \frac{\exp(z_i/T)}{\sum_{j=1}^{|z|} \exp(z_j/T)} \right) \tag{9}
$    

$
\mathcal{L}_{\text{KD}} = - \lambda \log\left( \frac{\exp(z_y)}{\sum_{j=1}^{|z|} \exp(z_j)} \right) + (1 - \lambda) \mathcal{L}_{\text{match}} \quad \text{s.t. } 0 \leq \lambda \leq 1 \tag{10}
$    




## 3. Conclusion





## 4. Reference
blog :     

paper : [Network Pruning via Transformable Architecture Search](https://arxiv.org/abs/1905.09717)

